{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:/PublicationWork/Alok-Mastor/Dataset/parkinsons_updrs.csv')\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "nan_values = data.isna().sum()\n",
    "# print(nan_values)\n",
    "\n",
    "# Correct NaN values by filling them with appropriate values\n",
    "# data.fillna(value, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = data.skew()\n",
    "# print(skewness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'data' is the DataFrame containing your dataset\n",
    "column_names = list(data.columns)\n",
    "\n",
    "outliers = []\n",
    "for col_name in column_names:\n",
    "    # Calculate the IQR for the column\n",
    "    q1 = data[col_name].quantile(0.25)\n",
    "    q3 = data[col_name].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Define the upper and lower bounds for outliers\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "    # Find the outliers in the column\n",
    "    column_outliers = data[(data[col_name] < lower_bound) | (data[col_name] > upper_bound)]\n",
    "    outliers.append(column_outliers)\n",
    "\n",
    "# Print the outliers for each column\n",
    "# for i, col_name in enumerate(column_names):\n",
    "#     print(f\"Outliers in column '{col_name}':\")\n",
    "#     print(outliers[i])\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_names = list(data.columns)\n",
    "column_names = column_names[:-1]\n",
    "\n",
    "skewedCols = []\n",
    "for names in column_names:\n",
    "    skewVal = data[names].skew()\n",
    "    if(skewVal>1 or skewVal<-1):\n",
    "        skewedCols.append(names)\n",
    "# print(skewedCols)\n",
    "# print(len(skewedCols))\n",
    "\n",
    "#   Name of the cols containing atleast one negative value\n",
    "skewedCols_NegativeVals = []\n",
    "for col_name in skewedCols:\n",
    "    for values in data[col_name]:\n",
    "        if(values < 0):\n",
    "            skewedCols_NegativeVals.append(col_name)\n",
    "            break\n",
    "# print(len(skewedCols_NegativeVals))\n",
    "\n",
    "#   Name of the cols containing atleast one zero values\n",
    "skewedCols_ZeroVals = []\n",
    "for col_name in skewedCols:\n",
    "    if col_name in skewedCols_NegativeVals:\n",
    "        continue\n",
    "    else:\n",
    "        for values in data[col_name]:\n",
    "            if(values == 0):\n",
    "                skewedCols_ZeroVals.append(col_name)\n",
    "                break\n",
    "# print(len(skewedCols_ZeroVals))\n",
    "\n",
    "#   Name of the cols containing only positive values\n",
    "skewedCols_PositiveVals = []\n",
    "for col_name in skewedCols:\n",
    "    if (col_name not in skewedCols_NegativeVals and col_name not in skewedCols_ZeroVals):\n",
    "        skewedCols_PositiveVals.append(col_name)\n",
    "# print(len(skewedCols_PositiveVals))\n",
    "\n",
    "#   Box-Cox Transformation\n",
    "for col_name in skewedCols_PositiveVals:\n",
    "    data[col_name] = stats.boxcox(data[col_name],lmbda=0)\n",
    "\n",
    "count = 0\n",
    "for col_name in skewedCols_PositiveVals:\n",
    "    skewVal = data[col_name].skew()\n",
    "    if skewVal>1 or skewVal<-1:\n",
    "        count = count + 1\n",
    "# print(count)\n",
    "# print(data)\n",
    "\n",
    "#   CubeRoot Transformation on skewedCols_NegativeVals & skewedCols_ZeroVals\n",
    "for col_name in skewedCols_NegativeVals:\n",
    "    data[col_name] = np.cbrt(data[col_name])\n",
    "for col_name in skewedCols_ZeroVals:\n",
    "    data[col_name] = np.cbrt(data[col_name])\n",
    "\n",
    "count = 0\n",
    "for col_name in skewedCols_NegativeVals:\n",
    "    skewVal = data[col_name].skew()\n",
    "    if skewVal>1 or skewVal<-1:\n",
    "        count = count + 1\n",
    "# print(count)\n",
    "# print(data)\n",
    "\n",
    "import statistics\n",
    "for col_name in column_names:\n",
    "    qi = data[col_name].quantile(0.25)\n",
    "    qf = data[col_name].quantile(0.75)\n",
    "\n",
    "    iqr = qf - qi\n",
    "    c = 1.5\n",
    "    upper_limit=qf+c*iqr\n",
    "    lower_limit=qi-c*iqr\n",
    "\n",
    "    col_median = statistics.mean(data[col_name].to_numpy())\n",
    "    for val in data[col_name]:\n",
    "        if val < lower_limit or val > upper_limit:\n",
    "            data[col_name] = data[col_name].replace(val,col_median)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['severity'] = data['motor_UPDRS'].apply(lambda x: 0 if 0 < x < 20 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5875, 16)\n",
      "(5875,)\n"
     ]
    }
   ],
   "source": [
    "Y1 = data['severity']\n",
    "Y2 = data['motor_UPDRS']\n",
    "X = data.drop(['subject#', 'total_UPDRS', 'motor_UPDRS', 'age','sex','test_time','severity'], axis=1)\n",
    "print(X.shape)\n",
    "print(Y1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8122\n",
      "Test Accuracy: 0.7585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y1_train, Y1_test = train_test_split(X, Y1, test_size=0.1, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the MLP classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(10, 20, 10), max_iter=10000, alpha=0.001, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train_scaled, Y1_train)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "Y1_train_pred = clf.predict(X_train_scaled)\n",
    "\n",
    "# Calculate the training accuracy\n",
    "accuracy_train = accuracy_score(Y1_train, Y1_train_pred)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "Y1_test_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the test accuracy\n",
    "accuracy_test = accuracy_score(Y1_test, Y1_test_pred)\n",
    "\n",
    "# Print the training and test accuracy\n",
    "print(f'Training Accuracy: {accuracy_train:.4f}')\n",
    "print(f'Test Accuracy: {accuracy_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/10000], Loss: 0.6911, Training Accuracy: 0.5309, Testing Accuracy: 0.5238\n",
      "Epoch [100/10000], Loss: 0.6842, Training Accuracy: 0.5519, Testing Accuracy: 0.5323\n",
      "Epoch [150/10000], Loss: 0.6797, Training Accuracy: 0.5610, Testing Accuracy: 0.5323\n",
      "Epoch [200/10000], Loss: 0.6779, Training Accuracy: 0.5661, Testing Accuracy: 0.5289\n",
      "Epoch [250/10000], Loss: 0.6756, Training Accuracy: 0.5720, Testing Accuracy: 0.5340\n",
      "Epoch [300/10000], Loss: 0.6723, Training Accuracy: 0.5858, Testing Accuracy: 0.5323\n",
      "Epoch [350/10000], Loss: 0.6678, Training Accuracy: 0.5890, Testing Accuracy: 0.5578\n",
      "Epoch [400/10000], Loss: 0.6632, Training Accuracy: 0.6015, Testing Accuracy: 0.5731\n",
      "Epoch [450/10000], Loss: 0.6593, Training Accuracy: 0.6054, Testing Accuracy: 0.5680\n",
      "Epoch [500/10000], Loss: 0.6565, Training Accuracy: 0.6094, Testing Accuracy: 0.5748\n",
      "Epoch [550/10000], Loss: 0.6546, Training Accuracy: 0.6208, Testing Accuracy: 0.5799\n",
      "Epoch [600/10000], Loss: 0.6526, Training Accuracy: 0.6164, Testing Accuracy: 0.5935\n",
      "Epoch [650/10000], Loss: 0.6510, Training Accuracy: 0.6193, Testing Accuracy: 0.5850\n",
      "Epoch [700/10000], Loss: 0.6497, Training Accuracy: 0.6211, Testing Accuracy: 0.5884\n",
      "Epoch [750/10000], Loss: 0.6487, Training Accuracy: 0.6325, Testing Accuracy: 0.5714\n",
      "Epoch [800/10000], Loss: 0.6471, Training Accuracy: 0.6285, Testing Accuracy: 0.5816\n",
      "Epoch [850/10000], Loss: 0.6464, Training Accuracy: 0.6257, Testing Accuracy: 0.6020\n",
      "Epoch [900/10000], Loss: 0.6442, Training Accuracy: 0.6319, Testing Accuracy: 0.6003\n",
      "Epoch [950/10000], Loss: 0.6426, Training Accuracy: 0.6374, Testing Accuracy: 0.5986\n",
      "Epoch [1000/10000], Loss: 0.6413, Training Accuracy: 0.6372, Testing Accuracy: 0.6020\n",
      "Epoch [1050/10000], Loss: 0.6396, Training Accuracy: 0.6448, Testing Accuracy: 0.6071\n",
      "Epoch [1100/10000], Loss: 0.6381, Training Accuracy: 0.6435, Testing Accuracy: 0.6020\n",
      "Epoch [1150/10000], Loss: 0.6367, Training Accuracy: 0.6444, Testing Accuracy: 0.6037\n",
      "Epoch [1200/10000], Loss: 0.6352, Training Accuracy: 0.6501, Testing Accuracy: 0.6122\n",
      "Epoch [1250/10000], Loss: 0.6341, Training Accuracy: 0.6472, Testing Accuracy: 0.6088\n",
      "Epoch [1300/10000], Loss: 0.6330, Training Accuracy: 0.6488, Testing Accuracy: 0.6122\n",
      "Epoch [1350/10000], Loss: 0.6323, Training Accuracy: 0.6558, Testing Accuracy: 0.6293\n",
      "Epoch [1400/10000], Loss: 0.6309, Training Accuracy: 0.6546, Testing Accuracy: 0.6224\n",
      "Epoch [1450/10000], Loss: 0.6301, Training Accuracy: 0.6563, Testing Accuracy: 0.6224\n",
      "Epoch [1500/10000], Loss: 0.6292, Training Accuracy: 0.6580, Testing Accuracy: 0.6241\n",
      "Epoch [1550/10000], Loss: 0.6285, Training Accuracy: 0.6595, Testing Accuracy: 0.6276\n",
      "Epoch [1600/10000], Loss: 0.6277, Training Accuracy: 0.6594, Testing Accuracy: 0.6259\n",
      "Epoch [1650/10000], Loss: 0.6270, Training Accuracy: 0.6586, Testing Accuracy: 0.6241\n",
      "Epoch [1700/10000], Loss: 0.6264, Training Accuracy: 0.6592, Testing Accuracy: 0.6224\n",
      "Epoch [1750/10000], Loss: 0.6258, Training Accuracy: 0.6618, Testing Accuracy: 0.6259\n",
      "Epoch [1800/10000], Loss: 0.6252, Training Accuracy: 0.6631, Testing Accuracy: 0.6310\n",
      "Epoch [1850/10000], Loss: 0.6246, Training Accuracy: 0.6654, Testing Accuracy: 0.6310\n",
      "Epoch [1900/10000], Loss: 0.6242, Training Accuracy: 0.6648, Testing Accuracy: 0.6310\n",
      "Epoch [1950/10000], Loss: 0.6239, Training Accuracy: 0.6650, Testing Accuracy: 0.6310\n",
      "Epoch [2000/10000], Loss: 0.6233, Training Accuracy: 0.6645, Testing Accuracy: 0.6293\n",
      "Epoch [2050/10000], Loss: 0.6228, Training Accuracy: 0.6664, Testing Accuracy: 0.6276\n",
      "Epoch [2100/10000], Loss: 0.6224, Training Accuracy: 0.6662, Testing Accuracy: 0.6293\n",
      "Epoch [2150/10000], Loss: 0.6221, Training Accuracy: 0.6673, Testing Accuracy: 0.6412\n",
      "Epoch [2200/10000], Loss: 0.6220, Training Accuracy: 0.6665, Testing Accuracy: 0.6446\n",
      "Epoch [2250/10000], Loss: 0.6213, Training Accuracy: 0.6684, Testing Accuracy: 0.6378\n",
      "Epoch [2300/10000], Loss: 0.6209, Training Accuracy: 0.6686, Testing Accuracy: 0.6259\n",
      "Epoch [2350/10000], Loss: 0.6206, Training Accuracy: 0.6703, Testing Accuracy: 0.6395\n",
      "Epoch [2400/10000], Loss: 0.6203, Training Accuracy: 0.6713, Testing Accuracy: 0.6395\n",
      "Epoch [2450/10000], Loss: 0.6200, Training Accuracy: 0.6679, Testing Accuracy: 0.6361\n",
      "Epoch [2500/10000], Loss: 0.6198, Training Accuracy: 0.6707, Testing Accuracy: 0.6395\n",
      "Epoch [2550/10000], Loss: 0.6196, Training Accuracy: 0.6667, Testing Accuracy: 0.6395\n",
      "Epoch [2600/10000], Loss: 0.6197, Training Accuracy: 0.6716, Testing Accuracy: 0.6429\n",
      "Epoch [2650/10000], Loss: 0.6189, Training Accuracy: 0.6722, Testing Accuracy: 0.6446\n",
      "Epoch [2700/10000], Loss: 0.6187, Training Accuracy: 0.6722, Testing Accuracy: 0.6446\n",
      "Epoch [2750/10000], Loss: 0.6186, Training Accuracy: 0.6686, Testing Accuracy: 0.6361\n",
      "Epoch [2800/10000], Loss: 0.6185, Training Accuracy: 0.6690, Testing Accuracy: 0.6395\n",
      "Epoch [2850/10000], Loss: 0.6181, Training Accuracy: 0.6716, Testing Accuracy: 0.6412\n",
      "Epoch [2900/10000], Loss: 0.6180, Training Accuracy: 0.6705, Testing Accuracy: 0.6412\n",
      "Epoch [2950/10000], Loss: 0.6178, Training Accuracy: 0.6703, Testing Accuracy: 0.6429\n",
      "Epoch [3000/10000], Loss: 0.6181, Training Accuracy: 0.6705, Testing Accuracy: 0.6378\n",
      "Epoch [3050/10000], Loss: 0.6178, Training Accuracy: 0.6705, Testing Accuracy: 0.6378\n",
      "Epoch [3100/10000], Loss: 0.6174, Training Accuracy: 0.6730, Testing Accuracy: 0.6412\n",
      "Epoch [3150/10000], Loss: 0.6172, Training Accuracy: 0.6724, Testing Accuracy: 0.6514\n",
      "Epoch [3200/10000], Loss: 0.6172, Training Accuracy: 0.6730, Testing Accuracy: 0.6497\n",
      "Epoch [3250/10000], Loss: 0.6169, Training Accuracy: 0.6732, Testing Accuracy: 0.6395\n",
      "Epoch [3300/10000], Loss: 0.6168, Training Accuracy: 0.6726, Testing Accuracy: 0.6395\n",
      "Epoch [3350/10000], Loss: 0.6167, Training Accuracy: 0.6724, Testing Accuracy: 0.6497\n",
      "Epoch [3400/10000], Loss: 0.6168, Training Accuracy: 0.6726, Testing Accuracy: 0.6395\n",
      "Epoch [3450/10000], Loss: 0.6165, Training Accuracy: 0.6745, Testing Accuracy: 0.6395\n",
      "Epoch [3500/10000], Loss: 0.6164, Training Accuracy: 0.6741, Testing Accuracy: 0.6378\n",
      "Epoch [3550/10000], Loss: 0.6162, Training Accuracy: 0.6733, Testing Accuracy: 0.6429\n",
      "Epoch [3600/10000], Loss: 0.6161, Training Accuracy: 0.6733, Testing Accuracy: 0.6429\n",
      "Epoch [3650/10000], Loss: 0.6161, Training Accuracy: 0.6737, Testing Accuracy: 0.6395\n",
      "Epoch [3700/10000], Loss: 0.6159, Training Accuracy: 0.6739, Testing Accuracy: 0.6412\n",
      "Epoch [3750/10000], Loss: 0.6159, Training Accuracy: 0.6733, Testing Accuracy: 0.6412\n",
      "Epoch [3800/10000], Loss: 0.6158, Training Accuracy: 0.6733, Testing Accuracy: 0.6463\n",
      "Epoch [3850/10000], Loss: 0.6156, Training Accuracy: 0.6741, Testing Accuracy: 0.6514\n",
      "Epoch [3900/10000], Loss: 0.6156, Training Accuracy: 0.6732, Testing Accuracy: 0.6429\n",
      "Epoch [3950/10000], Loss: 0.6157, Training Accuracy: 0.6726, Testing Accuracy: 0.6378\n",
      "Epoch [4000/10000], Loss: 0.6154, Training Accuracy: 0.6747, Testing Accuracy: 0.6429\n",
      "Epoch [4050/10000], Loss: 0.6155, Training Accuracy: 0.6730, Testing Accuracy: 0.6531\n",
      "Epoch [4100/10000], Loss: 0.6156, Training Accuracy: 0.6733, Testing Accuracy: 0.6361\n",
      "Epoch [4150/10000], Loss: 0.6152, Training Accuracy: 0.6733, Testing Accuracy: 0.6548\n",
      "Epoch [4200/10000], Loss: 0.6151, Training Accuracy: 0.6739, Testing Accuracy: 0.6344\n",
      "Epoch [4250/10000], Loss: 0.6150, Training Accuracy: 0.6730, Testing Accuracy: 0.6344\n",
      "Epoch [4300/10000], Loss: 0.6149, Training Accuracy: 0.6739, Testing Accuracy: 0.6514\n",
      "Epoch [4350/10000], Loss: 0.6152, Training Accuracy: 0.6726, Testing Accuracy: 0.6378\n",
      "Epoch [4400/10000], Loss: 0.6147, Training Accuracy: 0.6745, Testing Accuracy: 0.6531\n",
      "Epoch [4450/10000], Loss: 0.6147, Training Accuracy: 0.6733, Testing Accuracy: 0.6361\n",
      "Epoch [4500/10000], Loss: 0.6147, Training Accuracy: 0.6737, Testing Accuracy: 0.6531\n",
      "Epoch [4550/10000], Loss: 0.6148, Training Accuracy: 0.6728, Testing Accuracy: 0.6378\n",
      "Epoch [4600/10000], Loss: 0.6143, Training Accuracy: 0.6747, Testing Accuracy: 0.6446\n",
      "Epoch [4650/10000], Loss: 0.6145, Training Accuracy: 0.6735, Testing Accuracy: 0.6514\n",
      "Epoch [4700/10000], Loss: 0.6142, Training Accuracy: 0.6737, Testing Accuracy: 0.6514\n",
      "Epoch [4750/10000], Loss: 0.6140, Training Accuracy: 0.6749, Testing Accuracy: 0.6514\n",
      "Epoch [4800/10000], Loss: 0.6138, Training Accuracy: 0.6745, Testing Accuracy: 0.6446\n",
      "Epoch [4850/10000], Loss: 0.6137, Training Accuracy: 0.6754, Testing Accuracy: 0.6480\n",
      "Epoch [4900/10000], Loss: 0.6136, Training Accuracy: 0.6751, Testing Accuracy: 0.6480\n",
      "Epoch [4950/10000], Loss: 0.6135, Training Accuracy: 0.6751, Testing Accuracy: 0.6480\n",
      "Epoch [5000/10000], Loss: 0.6138, Training Accuracy: 0.6741, Testing Accuracy: 0.6361\n",
      "Epoch [5050/10000], Loss: 0.6132, Training Accuracy: 0.6758, Testing Accuracy: 0.6463\n",
      "Epoch [5100/10000], Loss: 0.6131, Training Accuracy: 0.6764, Testing Accuracy: 0.6446\n",
      "Epoch [5150/10000], Loss: 0.6131, Training Accuracy: 0.6739, Testing Accuracy: 0.6395\n",
      "Epoch [5200/10000], Loss: 0.6129, Training Accuracy: 0.6743, Testing Accuracy: 0.6395\n",
      "Epoch [5250/10000], Loss: 0.6132, Training Accuracy: 0.6747, Testing Accuracy: 0.6378\n",
      "Epoch [5300/10000], Loss: 0.6126, Training Accuracy: 0.6760, Testing Accuracy: 0.6412\n",
      "Epoch [5350/10000], Loss: 0.6124, Training Accuracy: 0.6773, Testing Accuracy: 0.6480\n",
      "Epoch [5400/10000], Loss: 0.6122, Training Accuracy: 0.6775, Testing Accuracy: 0.6412\n",
      "Epoch [5450/10000], Loss: 0.6120, Training Accuracy: 0.6777, Testing Accuracy: 0.6378\n",
      "Epoch [5500/10000], Loss: 0.6120, Training Accuracy: 0.6769, Testing Accuracy: 0.6378\n",
      "Epoch [5550/10000], Loss: 0.6124, Training Accuracy: 0.6785, Testing Accuracy: 0.6531\n",
      "Epoch [5600/10000], Loss: 0.6116, Training Accuracy: 0.6771, Testing Accuracy: 0.6429\n",
      "Epoch [5650/10000], Loss: 0.6116, Training Accuracy: 0.6768, Testing Accuracy: 0.6412\n",
      "Epoch [5700/10000], Loss: 0.6114, Training Accuracy: 0.6785, Testing Accuracy: 0.6412\n",
      "Epoch [5750/10000], Loss: 0.6116, Training Accuracy: 0.6754, Testing Accuracy: 0.6395\n",
      "Epoch [5800/10000], Loss: 0.6109, Training Accuracy: 0.6803, Testing Accuracy: 0.6378\n",
      "Epoch [5850/10000], Loss: 0.6112, Training Accuracy: 0.6811, Testing Accuracy: 0.6412\n",
      "Epoch [5900/10000], Loss: 0.6106, Training Accuracy: 0.6790, Testing Accuracy: 0.6378\n",
      "Epoch [5950/10000], Loss: 0.6104, Training Accuracy: 0.6785, Testing Accuracy: 0.6446\n",
      "Epoch [6000/10000], Loss: 0.6102, Training Accuracy: 0.6777, Testing Accuracy: 0.6361\n",
      "Epoch [6050/10000], Loss: 0.6114, Training Accuracy: 0.6807, Testing Accuracy: 0.6548\n",
      "Epoch [6100/10000], Loss: 0.6100, Training Accuracy: 0.6794, Testing Accuracy: 0.6463\n",
      "Epoch [6150/10000], Loss: 0.6097, Training Accuracy: 0.6798, Testing Accuracy: 0.6480\n",
      "Epoch [6200/10000], Loss: 0.6095, Training Accuracy: 0.6794, Testing Accuracy: 0.6429\n",
      "Epoch [6250/10000], Loss: 0.6092, Training Accuracy: 0.6803, Testing Accuracy: 0.6480\n",
      "Epoch [6300/10000], Loss: 0.6090, Training Accuracy: 0.6813, Testing Accuracy: 0.6480\n",
      "Epoch [6350/10000], Loss: 0.6088, Training Accuracy: 0.6819, Testing Accuracy: 0.6395\n",
      "Epoch [6400/10000], Loss: 0.6087, Training Accuracy: 0.6826, Testing Accuracy: 0.6480\n",
      "Epoch [6450/10000], Loss: 0.6093, Training Accuracy: 0.6745, Testing Accuracy: 0.6344\n",
      "Epoch [6500/10000], Loss: 0.6081, Training Accuracy: 0.6838, Testing Accuracy: 0.6497\n",
      "Epoch [6550/10000], Loss: 0.6091, Training Accuracy: 0.6732, Testing Accuracy: 0.6344\n",
      "Epoch [6600/10000], Loss: 0.6076, Training Accuracy: 0.6822, Testing Accuracy: 0.6480\n",
      "Epoch [6650/10000], Loss: 0.6074, Training Accuracy: 0.6849, Testing Accuracy: 0.6548\n",
      "Epoch [6700/10000], Loss: 0.6073, Training Accuracy: 0.6853, Testing Accuracy: 0.6565\n",
      "Epoch [6750/10000], Loss: 0.6069, Training Accuracy: 0.6824, Testing Accuracy: 0.6480\n",
      "Epoch [6800/10000], Loss: 0.6067, Training Accuracy: 0.6877, Testing Accuracy: 0.6599\n",
      "Epoch [6850/10000], Loss: 0.6065, Training Accuracy: 0.6868, Testing Accuracy: 0.6548\n",
      "Epoch [6900/10000], Loss: 0.6067, Training Accuracy: 0.6858, Testing Accuracy: 0.6582\n",
      "Epoch [6950/10000], Loss: 0.6060, Training Accuracy: 0.6870, Testing Accuracy: 0.6599\n",
      "Epoch [7000/10000], Loss: 0.6065, Training Accuracy: 0.6855, Testing Accuracy: 0.6565\n",
      "Epoch [7050/10000], Loss: 0.6056, Training Accuracy: 0.6887, Testing Accuracy: 0.6548\n",
      "Epoch [7100/10000], Loss: 0.6062, Training Accuracy: 0.6862, Testing Accuracy: 0.6565\n",
      "Epoch [7150/10000], Loss: 0.6053, Training Accuracy: 0.6877, Testing Accuracy: 0.6531\n",
      "Epoch [7200/10000], Loss: 0.6049, Training Accuracy: 0.6875, Testing Accuracy: 0.6616\n",
      "Epoch [7250/10000], Loss: 0.6047, Training Accuracy: 0.6872, Testing Accuracy: 0.6565\n",
      "Epoch [7300/10000], Loss: 0.6049, Training Accuracy: 0.6872, Testing Accuracy: 0.6548\n",
      "Epoch [7350/10000], Loss: 0.6045, Training Accuracy: 0.6883, Testing Accuracy: 0.6514\n",
      "Epoch [7400/10000], Loss: 0.6044, Training Accuracy: 0.6904, Testing Accuracy: 0.6633\n",
      "Epoch [7450/10000], Loss: 0.6040, Training Accuracy: 0.6894, Testing Accuracy: 0.6633\n",
      "Epoch [7500/10000], Loss: 0.6044, Training Accuracy: 0.6860, Testing Accuracy: 0.6497\n",
      "Epoch [7550/10000], Loss: 0.6037, Training Accuracy: 0.6889, Testing Accuracy: 0.6531\n",
      "Epoch [7600/10000], Loss: 0.6040, Training Accuracy: 0.6911, Testing Accuracy: 0.6650\n",
      "Epoch [7650/10000], Loss: 0.6037, Training Accuracy: 0.6917, Testing Accuracy: 0.6684\n",
      "Epoch [7700/10000], Loss: 0.6041, Training Accuracy: 0.6908, Testing Accuracy: 0.6599\n",
      "Epoch [7750/10000], Loss: 0.6030, Training Accuracy: 0.6913, Testing Accuracy: 0.6599\n",
      "Epoch [7800/10000], Loss: 0.6029, Training Accuracy: 0.6896, Testing Accuracy: 0.6599\n",
      "Epoch [7850/10000], Loss: 0.6027, Training Accuracy: 0.6909, Testing Accuracy: 0.6616\n",
      "Epoch [7900/10000], Loss: 0.6026, Training Accuracy: 0.6906, Testing Accuracy: 0.6633\n",
      "Epoch [7950/10000], Loss: 0.6026, Training Accuracy: 0.6902, Testing Accuracy: 0.6650\n",
      "Epoch [8000/10000], Loss: 0.6024, Training Accuracy: 0.6917, Testing Accuracy: 0.6616\n",
      "Epoch [8050/10000], Loss: 0.6023, Training Accuracy: 0.6917, Testing Accuracy: 0.6633\n",
      "Epoch [8100/10000], Loss: 0.6024, Training Accuracy: 0.6902, Testing Accuracy: 0.6582\n",
      "Epoch [8150/10000], Loss: 0.6023, Training Accuracy: 0.6902, Testing Accuracy: 0.6650\n",
      "Epoch [8200/10000], Loss: 0.6019, Training Accuracy: 0.6915, Testing Accuracy: 0.6599\n",
      "Epoch [8250/10000], Loss: 0.6021, Training Accuracy: 0.6919, Testing Accuracy: 0.6633\n",
      "Epoch [8300/10000], Loss: 0.6016, Training Accuracy: 0.6919, Testing Accuracy: 0.6616\n",
      "Epoch [8350/10000], Loss: 0.6015, Training Accuracy: 0.6925, Testing Accuracy: 0.6633\n",
      "Epoch [8400/10000], Loss: 0.6017, Training Accuracy: 0.6911, Testing Accuracy: 0.6650\n",
      "Epoch [8450/10000], Loss: 0.6018, Training Accuracy: 0.6900, Testing Accuracy: 0.6650\n",
      "Epoch [8500/10000], Loss: 0.6017, Training Accuracy: 0.6908, Testing Accuracy: 0.6667\n",
      "Epoch [8550/10000], Loss: 0.6011, Training Accuracy: 0.6925, Testing Accuracy: 0.6633\n",
      "Epoch [8600/10000], Loss: 0.6009, Training Accuracy: 0.6932, Testing Accuracy: 0.6633\n",
      "Epoch [8650/10000], Loss: 0.6012, Training Accuracy: 0.6908, Testing Accuracy: 0.6667\n",
      "Epoch [8700/10000], Loss: 0.6008, Training Accuracy: 0.6913, Testing Accuracy: 0.6633\n",
      "Epoch [8750/10000], Loss: 0.6007, Training Accuracy: 0.6938, Testing Accuracy: 0.6667\n",
      "Epoch [8800/10000], Loss: 0.6005, Training Accuracy: 0.6911, Testing Accuracy: 0.6633\n",
      "Epoch [8850/10000], Loss: 0.6004, Training Accuracy: 0.6936, Testing Accuracy: 0.6701\n",
      "Epoch [8900/10000], Loss: 0.6006, Training Accuracy: 0.6904, Testing Accuracy: 0.6718\n",
      "Epoch [8950/10000], Loss: 0.6003, Training Accuracy: 0.6917, Testing Accuracy: 0.6650\n",
      "Epoch [9000/10000], Loss: 0.6002, Training Accuracy: 0.6911, Testing Accuracy: 0.6633\n",
      "Epoch [9050/10000], Loss: 0.6004, Training Accuracy: 0.6925, Testing Accuracy: 0.6735\n",
      "Epoch [9100/10000], Loss: 0.6005, Training Accuracy: 0.6925, Testing Accuracy: 0.6633\n",
      "Epoch [9150/10000], Loss: 0.6005, Training Accuracy: 0.6921, Testing Accuracy: 0.6752\n",
      "Epoch [9200/10000], Loss: 0.5998, Training Accuracy: 0.6911, Testing Accuracy: 0.6650\n",
      "Epoch [9250/10000], Loss: 0.5998, Training Accuracy: 0.6909, Testing Accuracy: 0.6650\n",
      "Epoch [9300/10000], Loss: 0.5998, Training Accuracy: 0.6919, Testing Accuracy: 0.6701\n",
      "Epoch [9350/10000], Loss: 0.5994, Training Accuracy: 0.6913, Testing Accuracy: 0.6667\n",
      "Epoch [9400/10000], Loss: 0.6001, Training Accuracy: 0.6862, Testing Accuracy: 0.6582\n",
      "Epoch [9450/10000], Loss: 0.5993, Training Accuracy: 0.6908, Testing Accuracy: 0.6667\n",
      "Epoch [9500/10000], Loss: 0.6001, Training Accuracy: 0.6868, Testing Accuracy: 0.6582\n",
      "Epoch [9550/10000], Loss: 0.5993, Training Accuracy: 0.6881, Testing Accuracy: 0.6599\n",
      "Epoch [9600/10000], Loss: 0.5992, Training Accuracy: 0.6911, Testing Accuracy: 0.6667\n",
      "Epoch [9650/10000], Loss: 0.5987, Training Accuracy: 0.6934, Testing Accuracy: 0.6650\n",
      "Epoch [9700/10000], Loss: 0.5987, Training Accuracy: 0.6923, Testing Accuracy: 0.6684\n",
      "Epoch [9750/10000], Loss: 0.5985, Training Accuracy: 0.6932, Testing Accuracy: 0.6650\n",
      "Epoch [9800/10000], Loss: 0.5983, Training Accuracy: 0.6913, Testing Accuracy: 0.6650\n",
      "Epoch [9850/10000], Loss: 0.5983, Training Accuracy: 0.6902, Testing Accuracy: 0.6616\n",
      "Epoch [9900/10000], Loss: 0.5991, Training Accuracy: 0.6860, Testing Accuracy: 0.6599\n",
      "Epoch [9950/10000], Loss: 0.5980, Training Accuracy: 0.6936, Testing Accuracy: 0.6616\n",
      "Epoch [10000/10000], Loss: 0.5984, Training Accuracy: 0.6923, Testing Accuracy: 0.6667\n",
      "Best Model, Epoch [9989/10000], Loss: 0.5979, Training Accuracy: 0.6923, Testing Accuracy: 0.6650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the neural network model\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, input_size, h1,h2,h3, output_size):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, h1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(h2, h3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(h3, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x= self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x= self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y1_train, Y1_test = train_test_split(X, Y1, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Y1_train_tensor = torch.tensor(Y1_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y1_test_tensor = torch.tensor(Y1_test.values, dtype=torch.long)\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "h1=10\n",
    "h2=20\n",
    "h3=10\n",
    "output_size = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10000\n",
    "\n",
    "# Create an instance of the classification model\n",
    "model = ClassificationModel(input_size, h1,h2,h3, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Forward pass\n",
    "#     outputs = model(X_train_tensor)\n",
    "#     loss = criterion(outputs, Y1_train_tensor)\n",
    "\n",
    "#     # Backward and optimize\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Calculate training accuracy\n",
    "#     _, predicted_labels_train = torch.max(outputs, 1)\n",
    "#     accuracy_train = (predicted_labels_train == Y1_train_tensor).sum().item() / Y1_train_tensor.size(0)\n",
    "\n",
    "#     # Print the loss and training accuracy for every 50 epochs\n",
    "#     if (epoch+1) % 50 == 0:\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Training Accuracy: {accuracy_train:.4f}')\n",
    "\n",
    "# # Evaluate the model on the testing set\n",
    "# with torch.no_grad():\n",
    "#     predicted_Y1 = model(X_test_tensor)\n",
    "#     _, predicted_labels = torch.max(predicted_Y1, 1)\n",
    "#     accuracy = (predicted_labels == Y1_test_tensor).sum().item() / Y1_test_tensor.size(0)\n",
    "#     print(f'Test Accuracy: {accuracy:.4f}')\n",
    "# Initialize variables to keep track of the best model and its metrics\n",
    "best_model = None\n",
    "best_epoch = -1\n",
    "best_loss = float('inf')\n",
    "best_accuracy_train = 0\n",
    "best_accuracy_test = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, Y1_train_tensor)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    _, predicted_labels_train = torch.max(outputs, 1)\n",
    "    accuracy_train = (predicted_labels_train == Y1_train_tensor).sum().item() / Y1_train_tensor.size(0)\n",
    "\n",
    "    # Evaluate on the testing set\n",
    "    with torch.no_grad():\n",
    "        predicted_Y1 = model(X_test_tensor)\n",
    "        _, predicted_labels = torch.max(predicted_Y1, 1)\n",
    "        accuracy_test = (predicted_labels == Y1_test_tensor).sum().item() / Y1_test_tensor.size(0)\n",
    "\n",
    "    # Check if this model is better than the previous best model\n",
    "    if loss.item() < best_loss:\n",
    "        best_model = model\n",
    "        best_epoch = epoch\n",
    "        best_loss = loss.item()\n",
    "        best_accuracy_train = accuracy_train\n",
    "        best_accuracy_test = accuracy_test\n",
    "\n",
    "    # Print the loss and training and testing accuracy for every 50 epochs\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Training Accuracy: {accuracy_train:.4f}, Testing Accuracy: {accuracy_test:.4f}')\n",
    "\n",
    "# Print the metrics for the best model\n",
    "print(f'Best Model, Epoch [{best_epoch+1}/{num_epochs}], Loss: {best_loss:.4f}, Training Accuracy: {best_accuracy_train:.4f}, Testing Accuracy: {best_accuracy_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y1_train, Y1_test = train_test_split(X, Y1, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "model = SVC()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, Y1_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "Y1_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(Y1_test, Y1_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 9.2195\n",
      "R2 Score: 0.2042\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y1_train, Y1_test = train_test_split(X, Y1, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert the data to DMatrix format\n",
    "dtrain = xgb.DMatrix(X_train, label=Y1_train)\n",
    "dtest = xgb.DMatrix(X_test, label=Y1_test)\n",
    "\n",
    "# Set the hyperparameters for the XGBoost regressor\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'gamma': 0,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# Train the XGBoost regressor\n",
    "num_rounds = 100\n",
    "model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "# Calculate the root mean squared error (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(Y1_test, y_pred))\n",
    "print(f'Root Mean Squared Error: {rmse:.4f}')\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2 = r2_score(Y1_test, y_pred)\n",
    "print(f'R2 Score: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000], Loss: 128.7202, Training R2: -0.2253\n",
      "Epoch [100/1000], Loss: 119.0726, Training R2: -0.0881\n",
      "Epoch [150/1000], Loss: 117.7274, Training R2: -0.0559\n",
      "Epoch [200/1000], Loss: 117.0648, Training R2: -0.0407\n",
      "Epoch [250/1000], Loss: 116.7153, Training R2: -0.0295\n",
      "Epoch [300/1000], Loss: 116.4413, Training R2: -0.0226\n",
      "Epoch [350/1000], Loss: 116.2805, Training R2: -0.0180\n",
      "Epoch [400/1000], Loss: 116.1499, Training R2: -0.0182\n",
      "Epoch [450/1000], Loss: 116.0157, Training R2: -0.0200\n",
      "Epoch [500/1000], Loss: 115.8666, Training R2: -0.0210\n",
      "Epoch [550/1000], Loss: 115.7100, Training R2: -0.0200\n",
      "Epoch [600/1000], Loss: 115.7758, Training R2: -0.0208\n",
      "Epoch [650/1000], Loss: 115.5608, Training R2: -0.0170\n",
      "Epoch [700/1000], Loss: 121.8288, Training R2: -0.0786\n",
      "Epoch [750/1000], Loss: 115.5449, Training R2: -0.0172\n",
      "Epoch [800/1000], Loss: 115.4399, Training R2: -0.0129\n",
      "Epoch [850/1000], Loss: 115.3915, Training R2: -0.0117\n",
      "Epoch [900/1000], Loss: 115.3882, Training R2: -0.0095\n",
      "Epoch [950/1000], Loss: 115.4876, Training R2: -0.0116\n",
      "Epoch [1000/1000], Loss: 115.3690, Training R2: -0.0085\n",
      "Epoch [50/1000], Loss: 115.3411, Training R2: -0.0077\n",
      "Epoch [100/1000], Loss: 115.3254, Training R2: -0.0066\n",
      "Epoch [150/1000], Loss: 115.3153, Training R2: -0.0054\n",
      "Epoch [200/1000], Loss: 115.5430, Training R2: -0.0069\n",
      "Epoch [250/1000], Loss: 115.3477, Training R2: -0.0060\n",
      "Epoch [300/1000], Loss: 115.3203, Training R2: -0.0047\n",
      "Epoch [350/1000], Loss: 115.3103, Training R2: -0.0041\n",
      "Epoch [400/1000], Loss: 115.3055, Training R2: -0.0035\n",
      "Epoch [450/1000], Loss: 115.3026, Training R2: -0.0029\n",
      "Epoch [500/1000], Loss: 123.0912, Training R2: -0.0632\n",
      "Epoch [550/1000], Loss: 115.3559, Training R2: -0.0055\n",
      "Epoch [600/1000], Loss: 115.3107, Training R2: -0.0028\n",
      "Epoch [650/1000], Loss: 115.3033, Training R2: -0.0026\n",
      "Epoch [700/1000], Loss: 115.3007, Training R2: -0.0022\n",
      "Epoch [750/1000], Loss: 115.2994, Training R2: -0.0018\n",
      "Epoch [800/1000], Loss: 118.6006, Training R2: -0.0376\n",
      "Epoch [850/1000], Loss: 115.3390, Training R2: -0.0038\n",
      "Epoch [900/1000], Loss: 115.3046, Training R2: -0.0020\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m     88\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 89\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Update the list of last 10 losses\u001b[39;00m\n",
      "File \u001b[1;32mc:\\NewDriveD\\Anaconda\\anaconda3\\envs\\python_3.11\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\NewDriveD\\Anaconda\\anaconda3\\envs\\python_3.11\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define the neural network model\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size,h1, h2,h3, output_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, h1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(h2, h3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(h3, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x= self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x= self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y1_train, Y1_test = train_test_split(X, Y1, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Y1_train_tensor = torch.tensor(Y1_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y1_test_tensor = torch.tensor(Y1_test.values, dtype=torch.float32)\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "h1 = 32\n",
    "h2 = 16\n",
    "h3 = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.05\n",
    "num_epochs = 1000\n",
    "\n",
    "# Create an instance of the regression model\n",
    "model = RegressionModel(input_size,h1, h2, h3, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Forward pass\n",
    "#     outputs = model(X_train_tensor)\n",
    "#     loss = criterion(outputs, Y1_train_tensor)\n",
    "\n",
    "#     # Backward and optimize\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Print the loss for every 10 epochs\n",
    "#     if (epoch+1) % 50 == 0:\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# # Evaluate the model on the testing set\n",
    "# with torch.no_grad():\n",
    "#     predicted_Y1 = model(X_test_tensor)\n",
    "#     test_loss = criterion(predicted_Y1, Y1_test_tensor)\n",
    "#     print(f'Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Initialize a list to keep track of the loss for the last 10 epochs\n",
    "last_10_losses = [None] * 10\n",
    "\n",
    "while True:\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, Y1_train_tensor)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the list of last 10 losses\n",
    "        last_10_losses.pop(0)\n",
    "        last_10_losses.append(loss.item())\n",
    "\n",
    "        # Check if the loss for the last 10 epochs is the same\n",
    "        if len(set(last_10_losses)) == 1 and None not in last_10_losses:\n",
    "            print(f'Loss: {loss.item()} for last 10 epochs. Stopping...')\n",
    "            break\n",
    "        else:\n",
    "            # Calculate training R2 score\n",
    "            train_r2 = r2_score(Y1_train_tensor.detach().numpy(), outputs.detach().numpy())\n",
    "            if((epoch+1) % 50 == 0):\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Training R2: {train_r2:.4f}')\n",
    "            # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Training R2: {train_r2:.4f}')\n",
    "\n",
    "    # If the loop was broken, stop training\n",
    "    if len(set(last_10_losses)) == 1 and None not in last_10_losses:\n",
    "        break\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "with torch.no_grad():\n",
    "    predicted_Y1 = model(X_test_tensor)\n",
    "    test_loss = criterion(predicted_Y1, Y1_test_tensor)\n",
    "    # Calculate testing R2 score\n",
    "    test_r2 = r2_score(Y1_test_tensor.detach().numpy(), predicted_Y1.detach().numpy())\n",
    "    print(f'Test Loss: {test_loss.item():.4f}, Test R2: {test_r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "                                                                               \n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "                                                                               \n",
      "                                                                               \n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "                                                                               \n",
      "Best pipeline: XGBRegressor(input_matrix, learning_rate=0.1, max_depth=7, min_child_weight=20, n_estimators=100, n_jobs=1, objective=reg:squarederror, subsample=0.5, verbosity=0)\n",
      "Pipeline(steps=[('xgbregressor',\n",
      "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "                              colsample_bylevel=None, colsample_bynode=None,\n",
      "                              colsample_bytree=None, device=None,\n",
      "                              early_stopping_rounds=None,\n",
      "                              enable_categorical=False, eval_metric=None,\n",
      "                              feature_types=None, gamma=None, grow_policy=None,\n",
      "                              importance_type=None,\n",
      "                              interaction_constraints=None, learning_rate=0.1,\n",
      "                              max_bin=None, max_cat_threshold=None,\n",
      "                              max_cat_to_onehot=None, max_delta_step=None,\n",
      "                              max_depth=7, max_leaves=None, min_child_weight=20,\n",
      "                              missing=nan, monotone_constraints=None,\n",
      "                              multi_strategy=None, n_estimators=100, n_jobs=1,\n",
      "                              num_parallel_tree=None, random_state=None, ...))])\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y1_train, Y1_test = train_test_split(X, Y1, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create an instance of TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2)\n",
    "\n",
    "# Fit the TPOTRegressor on the training data\n",
    "tpot.fit(X_train, Y1_train)\n",
    "\n",
    "# Print the best pipeline found by TPOT\n",
    "print(tpot.fitted_pipeline_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
